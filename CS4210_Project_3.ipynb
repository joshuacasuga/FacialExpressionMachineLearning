{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "gSt02qeQNwz1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "espmBEL59RFT"
      },
      "outputs": [],
      "source": [
        "# load data from csv file\n",
        "train_data = pd.read_csv('train_data.csv', header=None)\n",
        "train_targets = pd.read_csv('train_target.csv', header=None)\n",
        "test_data = pd.read_csv('test_data.csv', header=None)\n",
        "\n",
        "# convert data to numpy arrays\n",
        "X_train = train_data.values.reshape(-1, 1, 48, 48).astype(np.float32)\n",
        "y_train = train_targets.values.squeeze().astype(np.int64)\n",
        "X_test = test_data.values.reshape(-1, 1, 48, 48).astype(np.float32)\n",
        "\n",
        "# normalize pixel values from [0, 255] to [0, 1]\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ensuring data is in the correct shape\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDXQM4ZgFST5",
        "outputId": "899aa302-4971-4de4-cc18-fdedbd53c409"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16175, 1, 48, 48)\n",
            "(16175,)\n",
            "(3965, 1, 48, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert y_train to one-hot encoding\n",
        "y_train_one_hot = F.one_hot(torch.tensor(y_train), num_classes=3).float()\n",
        "\n",
        "# convert all data to PyTorch Tensors\n",
        "X_train_tensor = torch.tensor(X_train)\n",
        "y_train_tensor = y_train_one_hot\n",
        "X_test_tensor = torch.tensor(X_test)\n",
        "\n",
        "# checking tensor types for debugging\n",
        "print(f'X_train_tensor type: {type(X_train_tensor)}, shape: {X_train_tensor.shape}')\n",
        "print(f'y_train_tensor type: {type(y_train_tensor)}, shape: {y_train_tensor.shape}')\n",
        "print(f'X_test_tensor type: {type(X_test_tensor)}, shape: {X_test_tensor.shape}')\n",
        "\n",
        "# creating tensor datasets and dataloaders\n",
        "train_tensor = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_tensor, batch_size=64, shuffle=True)\n",
        "\n",
        "test_tensor = TensorDataset(X_test_tensor)\n",
        "test_loader = DataLoader(test_tensor, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoTLVbBmzqtv",
        "outputId": "4fa6dfe9-47de-45f3-f4ec-c9e2c29064d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_tensor type: <class 'torch.Tensor'>, shape: torch.Size([16175, 1, 48, 48])\n",
            "y_train_tensor type: <class 'torch.Tensor'>, shape: torch.Size([16175, 3])\n",
            "X_test_tensor type: <class 'torch.Tensor'>, shape: torch.Size([3965, 1, 48, 48])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQyXdLEO00dl",
        "outputId": "baf87ff2-54a2-4499-9392-9f847cdf1da4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the model\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Dropout(0.25),\n",
        "\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Dropout(0.25),\n",
        "\n",
        "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Dropout(0.25),\n",
        "\n",
        "    nn.Flatten(),  # Flatten the tensor before fully connected layers\n",
        "    nn.Linear(128 * 6 * 6, 256), # 128*6*6 = 4608, the output size after the final pooling layer\n",
        "    nn.BatchNorm1d(256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(256, 128),\n",
        "    nn.BatchNorm1d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(128, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "\n",
        "    nn.Linear(64, 32),\n",
        "    nn.BatchNorm1d(32),\n",
        "    nn.ReLU(),\n",
        "\n",
        "    nn.Linear(32, 16),\n",
        "    nn.BatchNorm1d(16),\n",
        "    nn.ReLU(),\n",
        "\n",
        "    nn.Linear(16, 3)\n",
        ")"
      ],
      "metadata": {
        "id": "8FzT-T4yOPxd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFE58P5Fow3P",
        "outputId": "d16b4378-acc2-49aa-da6c-c07cab0ba4c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU()\n",
            "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (4): Dropout(p=0.25, inplace=False)\n",
            "  (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (7): ReLU()\n",
            "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (9): Dropout(p=0.25, inplace=False)\n",
            "  (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (12): ReLU()\n",
            "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (14): Dropout(p=0.25, inplace=False)\n",
            "  (15): Flatten(start_dim=1, end_dim=-1)\n",
            "  (16): Linear(in_features=4608, out_features=256, bias=True)\n",
            "  (17): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): ReLU()\n",
            "  (19): Dropout(p=0.5, inplace=False)\n",
            "  (20): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (21): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (22): ReLU()\n",
            "  (23): Dropout(p=0.5, inplace=False)\n",
            "  (24): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (25): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (26): ReLU()\n",
            "  (27): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (28): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (29): ReLU()\n",
            "  (30): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (31): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (32): ReLU()\n",
            "  (33): Linear(in_features=16, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss function, optimizer, and learning rate scheduler\n",
        "'''\n",
        "optimizer uses weight decay to penalize large weights\n",
        "scheduler reduces learning rate when validation loss plateaus\n",
        "'''\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)"
      ],
      "metadata": {
        "id": "8urYpf5gORpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8760a5-bb3d-40a1-ac28-029017948da9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with early stopping if model converges\n",
        "num_epochs = 50\n",
        "patience = 5\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "    scheduler.step(epoch_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "\n",
        "# predictions on test data\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images in test_loader:\n",
        "        images = images[0].to(torch.float32)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.extend(predicted.numpy())\n",
        "\n",
        "# submission file\n",
        "submission = pd.DataFrame({'Id': np.arange(len(predictions)), 'Category': predictions})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqFrQ_026-Oa",
        "outputId": "15fa0530-eb1c-47bd-fe7c-1e30960e7c5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.8006\n",
            "Epoch [2/50], Loss: 0.7268\n",
            "Epoch [3/50], Loss: 0.6787\n",
            "Epoch [4/50], Loss: 0.6527\n",
            "Epoch [5/50], Loss: 0.6198\n",
            "Epoch [6/50], Loss: 0.6031\n",
            "Epoch [7/50], Loss: 0.5892\n",
            "Epoch [8/50], Loss: 0.5695\n",
            "Epoch [9/50], Loss: 0.5508\n",
            "Epoch [10/50], Loss: 0.5370\n",
            "Epoch [11/50], Loss: 0.5254\n",
            "Epoch [12/50], Loss: 0.5152\n",
            "Epoch [13/50], Loss: 0.4930\n",
            "Epoch [14/50], Loss: 0.4877\n",
            "Epoch [15/50], Loss: 0.4751\n",
            "Epoch [16/50], Loss: 0.4512\n",
            "Epoch [17/50], Loss: 0.4483\n",
            "Epoch [18/50], Loss: 0.4383\n",
            "Epoch [19/50], Loss: 0.4211\n",
            "Epoch [20/50], Loss: 0.4135\n",
            "Epoch [21/50], Loss: 0.4011\n",
            "Epoch [22/50], Loss: 0.3855\n",
            "Epoch [23/50], Loss: 0.3768\n",
            "Epoch [24/50], Loss: 0.3654\n",
            "Epoch [25/50], Loss: 0.3564\n",
            "Epoch [26/50], Loss: 0.3525\n",
            "Epoch [27/50], Loss: 0.3348\n",
            "Epoch [28/50], Loss: 0.3354\n",
            "Epoch [29/50], Loss: 0.3224\n",
            "Epoch [30/50], Loss: 0.3173\n",
            "Epoch [31/50], Loss: 0.3139\n",
            "Epoch [32/50], Loss: 0.3043\n",
            "Epoch [33/50], Loss: 0.2978\n",
            "Epoch [34/50], Loss: 0.2855\n",
            "Epoch [35/50], Loss: 0.2857\n",
            "Epoch [36/50], Loss: 0.2803\n",
            "Epoch [37/50], Loss: 0.2780\n",
            "Epoch [38/50], Loss: 0.2740\n",
            "Epoch [39/50], Loss: 0.2636\n",
            "Epoch [40/50], Loss: 0.2586\n",
            "Epoch [41/50], Loss: 0.2579\n",
            "Epoch [42/50], Loss: 0.2603\n",
            "Epoch [43/50], Loss: 0.2519\n",
            "Epoch [44/50], Loss: 0.2449\n",
            "Epoch [45/50], Loss: 0.2337\n",
            "Epoch [46/50], Loss: 0.2385\n",
            "Epoch [47/50], Loss: 0.2439\n",
            "Epoch [48/50], Loss: 0.2267\n",
            "Epoch [49/50], Loss: 0.2215\n",
            "Epoch [50/50], Loss: 0.2313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions on test data\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images in test_loader:\n",
        "        images = images[0].to(torch.float32)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.extend(predicted.numpy())\n",
        "\n",
        "# submission file\n",
        "submission = pd.DataFrame({'Id': np.arange(len(predictions)), 'Category': predictions})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "GSDBmOJoJqyK"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}